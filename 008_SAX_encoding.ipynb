{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAX Encoding\n",
    "\n",
    "Questo notebook è stato ispirato da un [articolo letto mesi fa su KDNuggets](https://www.kdnuggets.com/2019/09/time-series-baseball.html) in cui si parla del SAX Encoding per trovare Time Series anomale.\n",
    "\n",
    "Nell'articolo viene usato un dataset sul baseball per trovare stagioni anomale per varie squadre, visto il momento in cui viene scritto provo a utilizzare il SAX Encoding sui [dati della protezione civile](https://github.com/pcm-dpc/COVID-19) per vedere se ci sono serie anomale tra i contagi da [*Covid-19*](https://it.wikipedia.org/wiki/COVID-19).\n",
    "\n",
    "* L'orizzonte temporale và dal 24/02/2020 al 03/04/2020.\n",
    "* La distribuzione geografica è per Regione anche se il Trentino - Alto Adige è diviso nelle due province autonome.\n",
    "\n",
    "### ATTENZIONE!!! IMPORTANTE.\n",
    "**Naturalmente non sono un medico/epidemiologo/virologo/... quindi il mio interesse era solo trovare un dataset che mi permettesse di spiegare come funziona l'algoritmo, non trarrò nessuna conclusione dai dati.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "dati_regioni = pd.read_csv(\"./dpc-covid19-ita-regioni.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dati_regioni.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La colonna che useremo sarà *totale_positivi* che, riporto direttamente dal readme del [repository ufficiale della protezione civile](https://github.com/pcm-dpc/COVID-19)\n",
    "\n",
    "> Totale attualmente positivi (ospedalizzati + isolamento domiciliare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totale_positivi = dati_regioni[[\"data\", \"denominazione_regione\", \"totale_positivi\"]].copy()\n",
    "totale_positivi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cos'è il SAX Encoding?\n",
    "\n",
    "Il SAX encoding è un metodo per semplificare le serie storiche, viene ridotta la dimensionalità col fine di trovare pattern anomali.\n",
    "\n",
    "Può essere considerato un metodo non supervisionato di *anomaly detection* dove per anomalia (o outlier) non si intende la singola osservazione ma l'intera serie storica.\n",
    "\n",
    "Le serie in questo caso sono tutte uguali, visto che non c'è assenza di dati, ma bisogna evidenziare che il SAX è una tecnica molto robusta ai valori mancanti e quindi utilizzabile quando si è in presenza di serie di lunghezza diversa, come nell'articolo sul baseball citato in precedenza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Standardizzare la serie\n",
    "\n",
    "* Per applicare questa tecnica bisogna avere la serie storica sulla riga e ad ogni colonna coinciderà uno step temporale.\n",
    "\n",
    "* Le serie devono essere standardizzate così da avere la stessa scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_pos_pivot = totale_positivi.pivot(index=\"data\", columns=\"denominazione_regione\", values=\"totale_positivi\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_pos_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_pos_pivot.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_pos_pivot.plot()\n",
    "#tot_pos_pivot.plot(legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stand = ((tot_pos_pivot.drop(\"data\", axis=1) - tot_pos_pivot.mean())/tot_pos_pivot.std()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora abbiamo la nostra matrice con sull'indice il nome della serie, ovvero la regione e sulle colonne il periodo da 0 a 39.\n",
    "\n",
    "### Step 2: Piecewise Aggregate Approximation\n",
    "\n",
    "Come accennato inzialmente questo metodo riduce la dimensionalità della serie e lo fa proprio con la P.A.A., per applicarla avremo bisogno di scegliere una finestra temporale, in questo caso visto che i vari ragionamenti (sentiti in TV, ribadisco di non avere nessuna competenza) si basano sulla settimana imposterò il parametro (window) *w=7*.\n",
    "\n",
    "Ridimensionata ogni serie in serie di *linghezza_serie*/*w* periodi si prende la media del periodo come nuovo valore.\n",
    "\n",
    "**Attenzione.** Bisogna impostare un controllo per non perdere informazioni, quindi **se il nuovo numero di periodi non è un intero và sempre arrotondato per eccesso.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = 7\n",
    "\n",
    "new_len = int(np.ceil((df_stand.shape[1]/windows)))\n",
    "\n",
    "# Crea il nuovo dataFrame\n",
    "df_PAA = pd.DataFrame(index = df_stand.index, columns = range(0, new_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola la media di ogni window\n",
    "ind = 0\n",
    "for i in range(0,df_stand.shape[1], windows):\n",
    "    avg = df_stand.iloc[:,i:i+windows].mean(axis=1)\n",
    "    df_PAA[ind] = avg.values\n",
    "    ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PAA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: SAX\n",
    "\n",
    "Il core di questa tecnica è che ora il dato numerico viene converito in stringa secondo dei livelli. La scelta dei livelli influisce sul risultato quindi bisognerebbe sempre farsi affiancare da un esperto di dominio.\n",
    "\n",
    "Io non conoscendo il fenomeno supporrò 3 livelli [\"A\", \"B\", \"C\"] e suddividerò i valori:\n",
    "* dal minimo fino al primo quartile\n",
    "* tra primo e terzo quartile\n",
    "* superiore al terzo quartile\n",
    "\n",
    "In questi caso potremmo interpretarli come:\n",
    "* pochi contagi, ottima situazione\n",
    "* situazione nella media\n",
    "* grave emergenza\n",
    "\n",
    "Una volta definiti i livelli questi vengono concatenati in un'unica stringa detta appunto \"*SAX string*\". Es. ABAAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sto ridondando con i DataFrame creati ma è per mostrare i vari step, sorry.\n",
    "binned = pd.DataFrame(index = df_PAA.index, columns = df_PAA.columns)\n",
    "\n",
    "for j in range(0, df_PAA.shape[1]):\n",
    "            bins = []\n",
    "            bins.append(df_PAA[j].min()-.01)\n",
    "            bins.append(df_PAA[j].quantile([0.25]).values[0])\n",
    "            bins.append(df_PAA[j].quantile([0.75]).values[0])\n",
    "            bins.append(df_PAA[j].max()+.01)\n",
    "            labels = [\"A\", \"B\", \"C\"]\n",
    "            binned[j] = pd.cut(df_PAA[j], bins, labels=labels)\n",
    "binned['sequence'] = binned.apply(''.join, axis=1)\n",
    "            \n",
    "binned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Quando un outlier è un outlier?\n",
    "\n",
    "In questo caso torna indispensabile la presenza di un esperto di dominio, io non essendolo (e lo scriverò ogni volta) ho fatto delle prove fino a trovare un risultato che mi permetta di mostrare qualcosa.\n",
    "\n",
    "Infatti una volta ottenute le SAX String bisogna contare le frequenze di ognuna e fissare una soglia per cui una serie è considerata anomala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imposto il limite a 1\n",
    "limit = 1\n",
    "\n",
    "# serie originale trasposta\n",
    "encoded = tot_pos_pivot.T\n",
    "encoded.drop(index=\"data\", inplace = True)\n",
    "\n",
    "# aggiungo l'etichetta se è un outlier o no\n",
    "freq = binned.sequence.value_counts()\n",
    "encoded['outlier'] = binned['sequence'].isin(list(freq[freq<=limit].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded[\"outlier\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da questo esperimento, considerando una finestra di 7 giorni e che un outlier per noi è chi ha una sequenza unica (viste anche le sole 21 serie) abbiamo 6 regioni che hanno lo stesso andamento e 15 che si discostano da tutte le altre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "for i in range(encoded.shape[0]):\n",
    "    if encoded.iloc[i,-1]:\n",
    "        col = 'r'\n",
    "    else:\n",
    "        col = 'b'\n",
    "    plt.plot(encoded.iloc[i,:-1], col)\n",
    "plt.legend()\n",
    "plt.title(\"Totale positivi per Regione dopo SAX Encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultima cosa da notare è che non sono solo i volumi ad influenzare questa tecnica ma anche gli andamenti come testimoniano le serie di Friuli e Lombardia che hanno volumi molto differenti ma probabilmente andamenti simili.\n",
    "\n",
    "*Se volete provare altre soluzioni potete provare a variare la finestra temporale e la soglia per essere considerato outlier*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bit36724f3ce08e41639a90b1f10ea73696"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
